{"cells":[{"cell_type":"markdown","metadata":{"id":"wkeRnBcBId2_"},"source":["Copyright 2021, Jeffrey Stanton. Apache License 2.0 (https://www.apache.org/licenses/LICENSE-2.0) feel free to create and distribute derivative works with attribution.\n","\n","This notebook demonstrates how to ingest a Paraphrase database (PPDB) phrasal equivalence file and then get it organized into a Pandas dataframe for further analysis. The notebook includes several diagnostic displays to help users better understand the data. For more details about PPDB, please consult:\n","\n","Pavlick, E., Rastogi, P., Ganitkevitch, J., Van Durme, B., & Callison-Burch, C. (2015, July). PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers) (pp. 425-430).\n","\n","You can download PPDB phrasal files from the PPDB website by clicking the appropriate buttons here: http://paraphrase.org/#/download\n","This code has been tested on the English small phrasal file, which unzips to about 1.35 GB. To test all of the steos in this notebook yourself, get a copy of the small phrasal file from the URL mentioned above, unzip it on your own computer, upload it to a convenient location on your Google drive and then change the pathname as noted in the third code block below. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUdlsdLlmqdB"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import time\n","\n","from sklearn.metrics.pairwise import cosine_similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVsnc9bIkPDP"},"outputs":[],"source":["# The PPDB raw files are quite large. This code assumes that you host\n","# the file on your Google drive. This code attaches your Google drive\n","# to the file store of your Colab notebook. The call to drive.mount() \n","# will request an auth code and will provide a link to help you get\n","# that code. Assuming you are logged in or can log in to your Google\n","# Drive account, you can copy and paste the auth code into a small\n","# box that will appear at the end of this cell.\n","from google.colab import drive\n","\n","drive_mounted = False\n","\n","if drive_mounted == False: # These save time if you click Run All again\n","    ppdb_imported = False\n","    drive.mount(\"/content/gdrive\", force_remount=True)\n","    drive_mounted = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XLBPql1bkoYi"},"outputs":[],"source":["# Choose a pathname within your Google drive. The easy way to do this \n","# on Colab is to use the file browser on the left to navigate to the\n","# file and then use the ... menu to copy the pathname. Replace the\n","# pathname shown in the next line of code.\n","pathname = '/content/gdrive/MyDrive/Data/ppdb-2.0-s-phrasal.txt'\n","\n","# Import the PPDB file from Google Drive: Takes up to half a minute.\n","if ppdb_imported == False and drive_mounted == True:\n","  with open(pathname, 'r') as f:\n","    ppdb = f.read()\n","  ppdb_imported = True # These save time if you click Run All again"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGwFQIepnAao"},"outputs":[],"source":["ppdb[:1000] # Each individual record is delimited by a newline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHmzj7Nsn8-O"},"outputs":[],"source":["# Create a list with one line/record per list entry\n","ppdb_lines = ppdb.split(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVdfNV8voD-K"},"outputs":[],"source":["type(ppdb_lines), len(ppdb_lines)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhZU6SNuqtSY"},"outputs":[],"source":["# Optionally remove ppdb to save RAM on small Jupyter VMs\n","del ppdb # Once we have parsed the lines, no need to keep the raw text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9SmToYnWlOHO"},"outputs":[],"source":["# This is an explorer routine to use to diagnose the contents of the PPDB. \n","# This code was developed to address the PPDB 2.0 phrasal dataset.\n","def fields_from_line(line):\n","  \"\"\"\n","  Show the field names from any line of a PPDB file. Expects a text string\n","  that contains one line from a PPDB file.\n","  \"\"\"\n","  # Set up a blank pandas dataframe with all of the fields we need\n","  col_list = [\"POS\", \"LHS\", \"RHS\"] # The first three fields\n","  att_str = line.split('|||')[3].strip() # All the junk after the first 3 fields\n","  # This extracts the first string in each field of the form name=1.2345\n","  col_list += [item.split(\"=\")[0] for item in att_str.split(\" \")] \n","\n","  return col_list\n"]},{"cell_type":"code","source":["ppdb_lines[1000]"],"metadata":{"id":"0uv6ndx4y_-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crvTN646lweW"},"outputs":[],"source":["fields_from_line(ppdb_lines[1000]) # Show the first line"]},{"cell_type":"markdown","metadata":{"id":"Qof_z19zBFpI"},"source":["These field definitions are mainly verbatim quoted from this supplement (https://aclanthology.org/attachments/P15-2070.Notes.pdf) as well as from various other papers published by Pavlick et al. The corresponding column that will be stored in a Pandas dataframe will contain numeric quantities but encoded as text data.\n","\n","* 'POS' - A part of speech tag sequence summarizing the LHS\n","* 'LHS' - The left hand side phrase (AKA source)\n","* 'RHS' - The right hand side phrase (AKA target)\n","* 'PPDB2.0Score' \n","* 'PPDB1.0Score' - the score used to rank paraphrases in the original release of PPDB, computed according to the heurisitic weighting given in the paper \n","* '-logp(LHS|e1)',\n","* '-logp(LHS|e2)',\n","* '-logp(e1|LHS)',\n","* '-logp(e1|e2)' - Negative log probability of e1 given e2 based on Bannard and Callison-Burch (2005)\n","* '-logp(e1|e2,LHS)',\n","* '-logp(e2|LHS)',\n","* '-logp(e2|e1)' - Negative log probability of e2 given e1 based on Bannard and Callison- Burch (2005)\n","* '-logp(e2|e1,LHS)',\n","* 'AGigaSim' - the distributional similarity of e1 and e2, computed according to contexts observed in the Annotated Gigaword corpus (Napoles et al., 2012)\n","* 'Abstract' - a binary feature that indicates whether the rule is composed exclusively of nonterminal symbols.\n","* 'Adjacent',\n","* 'CharCountDiff' - The difference in the number of characters in RHS versus LHS strings; negative when LHS is longer\n","* 'CharLogCR' - the log-compression ratio in characters,logchars(f2),a feature used in sentence compression\n","* 'ContainsX' - a binary feature that indicates whether the nonterminal symbol X (see Chiang, 2007)\n","* 'Equivalence' - predicted probability that the paraphrase pair represents semantic equiva- lence (e1 entails e2 and e2 entails e1), accord- ing to model used in Pavlick et al. (2015)\n","* 'Exclusion' - predicted probability that the paraphrase pair represents semantic exclusion\n","* 'GlueRule' - a binary feature that indicates whether this is a \"glue rule\" (see Post et al., 2013)\n","* 'GoogleNgramSim' - the distributional similarity of e1 and e2, computed according to contexts observed in the Google Ngram cor- pus (Brants and Franz, 2006)\n","* 'Identity' - a binary feature that indicates whether the phrase is identical to the paraphrase. Note that these should generally be excluded from an analysis.\n","* 'Independent' - predicted probability that the paraphrase pair represents semantic independence.\n","* 'Lex(e1|e2)' - the “lexical translation” probability of the RHS given the LHS (see Koehn et al., 2003)\n","* 'Lex(e2|e1)' - the “lexical translation” probability of the LHS given the RHS \n","* 'Lexical' - a binary feature that says whether this is a single word paraphrase\n","* 'LogCount' - the log of the frequency estimate for this paraphrase pair.\n","* 'MVLSASim' - Cosine similarity according to the Multiview Latent Semantic Analysis embeddings described by Rastogi et al. (2015)\n","* 'Monotonic' - a binary feature that indicates whether multiple nonterminal symbols occur in the same order (are monotonic) or if they are re-ordered\n","* 'OtherRelated' - predicted probability that the paraphrase pair represents topical relatedness but not entailment\n","* 'PhrasePenalty' - this feature is used by the decoder to count how many rules it uses in a derivation\n","* 'RarityPenalty' - marks rules that have only been seen a handful of times. It is calculated as exp(1 − c(e1,e2)), where c(e1 , e2 ) is the estimate of the frequency of this paraphrase pair\n","* 'ReverseEntailment' - predicted probability that the target phrase entails the source phrase; either this feature or the ForwardEntailment feature will be present, but not both\n","* 'SourceTerminalsButNoTarget' - a binary feature showing when the source phrase contains terminal symbols, but the target phrase contains no terminal symbols\n","* 'SourceWords' - The word count for the LHS phrase\n","* 'TargetTerminalsButNoSource' - a binary feature showing when the target phrase contains terminal symbols, but the source phrase contains no terminal symbols\n","* 'TargetWords' - The word count for the RHS phrase\n","* 'UnalignedSource' - a binary feature showing if there are any words in the source phrase that are not aligned to any words in the target phrase\n","* 'UnalignedTarget' - - a binary feature showing if there are any words in the target phrase that are not aligned to any words in the source phrase\n","* 'WordCountDiff' - The difference in word count between RHS and LHS phrases; a negative number means LHS phrase is longer\n","* 'WordLenDiff' - the difference in average word length between the source phrase and the target phrase\n","* 'WordLogCR' - the log-compression ratio in words, estimated as log words(e) words(f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5BIxrshACgk"},"outputs":[],"source":["# This function eats up a list of lines and turns it into a Pandas dataframe\n","# that will have the same fields as shown in the diagnostic above.\n","\n","\n","def load_ppdb_lines(lines):\n","    \"\"\"\n","    Load a paraphrase file from a list of input strings.\n","\n","    Note that the resulting pandas dataframe can be quite large and may occupy\n","    a substantial amount of RAM, depending on the number of records provided\n","    by the input.\n","\n","    :param lines: list of lines from the input file\n","    :return: a pandas data frame with POS, LHS, RHS and \n","    the various numeric attributes \n","    \"\"\"\n","\n","    rows = [] # We will build up a list of dictionaries that we will us to\n","              # construct the Pandas data frame at the end\n","\n","    for line in lines:\n","        \n","        # discard lines with unicode character encoding issues\n","        if '\\\\ x' in line or 'xc3' in line:\n","            continue\n","        \n","        fields = line.split('|||') # These are the major sections of each entry\n","        \n","        # Error check, make sure that there are two strings of text with at\n","        # least one character in each.\n","        if len(fields[0].strip()) == 0 or len(fields[1].strip()) == 0:\n","            continue\n","        \n","        tpd_dict = {\"POS\": fields[0].strip()} # Create a dictionary of available values\n","        tpd_dict.update( {\"LHS\" : fields[1].strip()} )\n","        tpd_dict.update( {\"RHS\" : fields[2].strip()} )\n","        \n","        attr = fields[3].strip() # Grab the list of statistics\n","\n","        for a in attr.split(\" \"): # Parse the list of values\n","          keyval = a.split(\"=\") # This yields the name of the attribute and its value\n","          tpd_dict.update( {keyval[0].strip(): keyval[1].strip()} )\n","\n","        # print(tpd_dict)\n","        rows.append(tpd_dict) # Add this dictionary to the list\n","\n","    return pd.DataFrame.from_dict(rows, orient='columns')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DnCQL31QJGgg"},"outputs":[],"source":["# This takes about a minute for a million records. Choose a value for max_lines\n","# suitable for your task. Uncomment the following line to read in the whole db.\n","# max_lines = len(ppdb_lines) # process whole dataset; note, uses much RAM\n","max_lines = 100000\n","\n","t0 = time.perf_counter() # Time the process\n","ppdb_df = load_ppdb_lines(ppdb_lines[:max_lines])\n","t1 = time.perf_counter()\n","print(\"Execution time per line:\", (t1 - t0)/max_lines * 1000 , \"milliseconds.\")\n","print(\"Total time:\", (t1 - t0), \"seconds.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pOozdTSqrkWD"},"outputs":[],"source":["# Optionally delete the ppdb_lines object to save RAM\n","del ppdb_lines"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peyLzJbYRUBH"},"outputs":[],"source":["ppdb_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"acaM766Vk-zZ"},"outputs":[],"source":["ppdb_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycblt9elj2iG"},"outputs":[],"source":["# Now proceed to whatever subsequent analysis is going to be done. Remember \n","# that the numeric fields are currently stored as text, so must be converted\n","# before analysis. For example, here is how to calc the correlation between \n","# PPDB 1.0 scores and PPDB 2.0 scores:\n","ppdb_df['ppdb1'] = pd.to_numeric(ppdb_df['PPDB1.0Score'], errors='coerce')\n","ppdb_df['ppdb2'] = pd.to_numeric(ppdb_df['PPDB2.0Score'], errors='coerce')\n","\n","ppdb_df['ppdb1'].corr(ppdb_df['ppdb2'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"65w-r1O0QAq_"},"outputs":[],"source":["# Let's convert all of the string columns containing numbers to actual numbers\n","columns = list(ppdb_df)\n","\n","for i in range(3, len(columns)):\n","  ppdb_df[columns[i]] = pd.to_numeric(ppdb_df[columns[i]], errors='coerce')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbnSyZ3asJc-"},"outputs":[],"source":["ppdb_df.to_csv(\"ppdb.csv\") # Save a csv file in case we would like to reload later\n","# If using colab, download this file before the VM shuts down."]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"ppdb.csv\")"],"metadata":{"id":"j1czUFM34VsL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eu4ViupkoNCx"},"outputs":[],"source":["# Now, optionally read the data structure back in if you are restarting from \n","# this point. Uncomment these lines to start here:\n","# import pandas as pd\n","pathname = '/content/gdrive/MyDrive/Data/ppdb.csv'\n","\n","ppdb_df = pd.read_csv(pathname)"]},{"cell_type":"markdown","metadata":{"id":"dWg3Lh0u1gqY"},"source":["From this point, one can use the brief texts stored in the Pandas dataframe to perform subsequent analyses. Some such analyses might be enhanced by also using the precalculated values that are also stored in the dataframe. For example, there are two measures of distance stored on the dataframe which could be used to compare with other measures of distance. Don't forget to adapt the code in the previous block to convert the attributes in the dataframe, which are stored as text, into numeric values. Note that some of the attribute fields contain instances of NaN, so you may have to wrestle with the fields or subset the data to condition it for your particular problem domain."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sx3hvVHz1OOg"},"outputs":[],"source":["def hist(dfcol):\n","  \"\"\"\n","  Here's a function that creates a histogram from a pd series, converting\n","  if necessary from a string to numeric.\n","\n","  :param dfcol: a column from a pandas dataframe\n","  :return: None\n","  \"\"\"\n","\n","  import matplotlib.pyplot as plt\n","  import numpy as np\n","  import pandas as pd\n","\n","  if len(dfcol) < 1:\n","    print(\"Error: Need at least one data point for histogram.\")\n","    return None\n","\n","  # Convert to numeric if it is a string, otherwise plt.hist()\n","  # will do a barplot using set(dfcol). \n","  #if type(dfcol[0]) == str:\n","  #  dfcol = pd.to_numeric(dfcol, errors='coerce')\n","\n","  plt.hist(dfcol)\n","\n","  return None\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"62CSmB6w_nqq"},"outputs":[],"source":["def scatter(dfcol1, dfcol2):\n","  \"\"\"\n","  Here's a function that creates a scattergram from two pd series, converting\n","  if necessary from string to numeric.\n","\n","  :param dfcol1: a column from a pandas dataframe\n","  :param dfcol2: a column from a pandas dataframe\n","  :return: None\n","  \"\"\"  \n","  \n","  import matplotlib.pyplot as plt\n","  import numpy as np\n","  import pandas as pd\n","\n","  if len(dfcol1) < 1:\n","    print(\"Error: Need at least one data point for scattergram.\")\n","    return None\n","\n","  if len(dfcol1) != len(dfcol2):\n","    print(\"Error: The two vectors are different lengths.\")\n","    return None\n","\n","  # Convert to numeric if it is a string, otherwise plt.hist()\n","  # will do a barplot using set(dfcol). \n","  #if type(dfcol1[0]) == str:\n","  #  dfcol1 = pd.to_numeric(dfcol1, errors='coerce')\n","\n","  #if type(dfcol2[0]) == str:\n","  #  dfcol2 = pd.to_numeric(dfcol2, errors='coerce')\n","\n","\n","  plt.scatter(dfcol1, dfcol2)\n","\n","  return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3CDdg_dpIcsa"},"outputs":[],"source":["def bicorr(dfcol1, dfcol2):\n","  \"\"\"\n","  Here's a function that creates a correlation from two pd series, converting\n","  if necessary from string to numeric.\n","\n","  :param dfcol1: a column from a pandas dataframe\n","  :param dfcol2: a column from a pandas dataframe\n","  :return: Pearson's correlation coefficient\n","  \"\"\"  \n","  import pandas as pd\n","  import numpy as np\n","\n","  if len(dfcol1) < 2:\n","    print(\"Error: Need at least two data points for correlation.\")\n","    return None\n","\n","  if len(dfcol1) != len(dfcol2):\n","    print(\"Error: The two vectors are different lengths.\")\n","    return None\n","\n","  # Convert to numeric if it is a string, \n","  #if type(dfcol1.dtype) == str:\n","  #  dfcol1 = pd.to_numeric(dfcol1, errors='coerce')\n","\n","  #if type(dfcol2.dtype) == str:\n","  #  dfcol2 = pd.to_numeric(dfcol2, errors='coerce')\n","\n","  #cm = np.corrcoef(dfcol1.values, dfcol2.values)\n","  #return cm[0,1]\n","\n","  return dfcol1.corr(dfcol2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hNxnO4LS1_gv"},"outputs":[],"source":["hist(ppdb_df['ppdb1'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FjdH8d-M4oG3"},"outputs":[],"source":["#hist(ppdb_df['ppdb2'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"retAQmRjCBiN"},"outputs":[],"source":["scatter(ppdb_df['ppdb1'], ppdb_df['ppdb2'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LdlGk1h-9-UE"},"outputs":[],"source":["#hist(ppdb_df['AGigaSim'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GciCdI8MARuO"},"outputs":[],"source":["#hist(ppdb_df['GoogleNgramSim'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5uimF6GUAbjT"},"outputs":[],"source":["#scatter(ppdb_df['AGigaSim'], ppdb_df['GoogleNgramSim'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5SL-QFAJC58"},"outputs":[],"source":["bicorr(ppdb_df['AGigaSim'], ppdb_df['GoogleNgramSim'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ba49YMspaF9c"},"outputs":[],"source":["# We can also sample texts from the main dataframe to create a new df\n","# Rather than create a list of dataframes, we will make a new df structure\n","# Takes about 2 minutes for 5000 samples\n","\n","text_samps = 1000\n","col_list = [\"LHS\", \"RHS\", \"RandText\", \"AGigaSim\", \"GoogleNgramSim\"]\n","\n","text_df = pd.DataFrame(columns=col_list) \n","\n","for i in range(text_samps):\n","  # Start with a single row randomly sampled; using seed to make results reproducible\n","  lrdf = ppdb_df.sample(n = 1, random_state=i)\n","  lhs = lrdf[\"LHS\"].values[0] # Peel off the first phrase\n","  rhs = lrdf[\"RHS\"].values[0] # Peel off the second phrase\n","  agiga = lrdf[\"AGigaSim\"].values[0] # Save the similarity\n","  googn = lrdf[\"GoogleNgramSim\"].values[0] # Save the similarity\n","\n","  lrdf2 = ppdb_df.sample(n = 1) # Now sample a second row to get an additional text\n","  randtext = lrdf2[\"RHS\"].values[0] # Save the second text\n","\n","  text_df = text_df.append(pd.DataFrame([[lhs, rhs, randtext, agiga, googn]], columns=col_list))\n","\n","len(text_df), type(text_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkNYuvRDfaw2"},"outputs":[],"source":["text_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_P8pmv5g3pd"},"outputs":[],"source":["!pip install sentence-transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_8p9jnwg0ag"},"outputs":[],"source":["from sentence_transformers import SentenceTransformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SfgJbttsjAWp"},"outputs":[],"source":["xflist = ['distiluse-base-multilingual-cased-v2',\n","'distiluse-base-multilingual-cased',\n","'distiluse-base-multilingual-cased-v1',\n","'paraphrase-multilingual-MiniLM-L12-v2',\n","'clip-ViT-B-32-multilingual-v1',\n","'all-mpnet-base-v2',\n","'clip-ViT-B-32',\n","'msmarco-bert-co-condensor',\n","'msmarco-distilbert-cos-v5',\n","'msmarco-MiniLM-L12-cos-v5',\n","'msmarco-MiniLM-L6-cos-v5',\n","'facebook-dpr-ctx_encoder-multiset-base',\n","'msmarco-bert-base-dot-v5',\n","'msmarco-distilbert-dot-v5',\n","'all-roberta-large-v1',\n","'paraphrase-distilroberta-base-v2',\n","'all-mpnet-base-v1',\n","'paraphrase-mpnet-base-v2',\n","'paraphrase-MiniLM-L12-v2',\n","'paraphrase-MiniLM-L6-v2',\n","'paraphrase-MiniLM-L3-v2',\n","'all-MiniLM-L12-v2',\n","'all-MiniLM-L12-v1',\n","'all-MiniLM-L6-v2',\n","'all-MiniLM-L6-v1',\n","'all-distilroberta-v1',\n","'multi-qa-mpnet-base-cos-v1',\n","'msmarco-distilbert-base-tas-b',\n","'multi-qa-mpnet-base-dot-v1',\n","'multi-qa-distilbert-cos-v1',\n","'multi-qa-MiniLM-L6-cos-v1',\n","'multi-qa-distilbert-dot-v1',\n","'multi-qa-MiniLM-L6-dot-v1',\n","'xlm-r-large-en-ko-nli-ststb',\n","'xlm-r-distilroberta-base-paraphrase-v1',\n","'xlm-r-bert-base-nli-stsb-mean-tokens',\n","'xlm-r-bert-base-nli-mean-tokens',\n","'xlm-r-base-en-ko-nli-ststb',\n","'xlm-r-100langs-bert-base-nli-stsb-mean-tokens',\n","'xlm-r-100langs-bert-base-nli-mean-tokens',\n","'stsb-xlm-r-multilingual',\n","'stsb-roberta-large',\n","'stsb-roberta-base',\n","'stsb-roberta-base-v2',\n","'stsb-mpnet-base-v2',\n","'stsb-distilroberta-base-v2',\n","'stsb-distilbert-base',\n","'stsb-bert-large',\n","'stsb-bert-base',\n","'roberta-large-nli-stsb-mean-tokens',\n","'roberta-large-nli-mean-tokens',\n","'roberta-base-nli-stsb-mean-tokens',\n","'roberta-base-nli-mean-tokens',\n","'quora-distilbert-multilingual',\n","'quora-distilbert-base',\n","'paraphrase-xlm-r-multilingual-v1',\n","'paraphrase-multilingual-mpnet-base-v2',\n","'paraphrase-distilroberta-base-v1',\n","'paraphrase-albert-small-v2',\n","'paraphrase-albert-base-v2',\n","'paraphrase-TinyBERT-L6-v2',\n","'nq-distilbert-base-v1',\n","'nli-roberta-large',\n","'nli-roberta-base',\n","'nli-roberta-base-v2',\n","'nli-mpnet-base-v2',\n","'nli-distilroberta-base-v2',\n","'nli-distilbert-base',\n","'nli-distilbert-base-max-pooling',\n","'nli-bert-large',\n","'nli-bert-large-max-pooling',\n","'nli-bert-large-cls-pooling',\n","'nli-bert-base',\n","'nli-bert-base-max-pooling',\n","'nli-bert-base-cls-pooling',\n","'msmarco-roberta-base-v3',\n","'msmarco-roberta-base-v2',\n","'msmarco-roberta-base-ance-firstp',\n","'msmarco-distilroberta-base-v2',\n","'msmarco-distilbert-multilingual-en-de-v2-tmp-trained-scratch',\n","'msmarco-distilbert-multilingual-en-de-v2-tmp-lng-aligned',\n","'msmarco-distilbert-base-v4',\n","'msmarco-distilbert-base-v3',\n","'msmarco-distilbert-base-v2',\n","'msmarco-distilbert-base-dot-prod-v3',\n","'msmarco-MiniLM-L-6-v3',\n","'msmarco-MiniLM-L-12-v3',\n","'facebook-dpr-question_encoder-single-nq-base',\n","'facebook-dpr-question_encoder-multiset-base',\n","'facebook-dpr-ctx_encoder-single-nq-base',\n","'distilroberta-base-paraphrase-v1',\n","'distilroberta-base-msmarco-v2',\n","'distilroberta-base-msmarco-v1',\n","'distilbert-multilingual-nli-stsb-quora-ranking',\n","'distilbert-base-nli-stsb-quora-ranking',\n","'distilbert-base-nli-stsb-mean-tokens',\n","'distilbert-base-nli-mean-tokens',\n","'distilbert-base-nli-max-tokens',\n","'bert-large-nli-stsb-mean-tokens',\n","'bert-large-nli-mean-tokens',\n","'bert-large-nli-max-tokens',\n","'bert-large-nli-cls-token',\n","'bert-base-wikipedia-sections-mean-tokens',\n","'bert-base-nli-stsb-mean-tokens',\n","'bert-base-nli-mean-tokens',\n","'bert-base-nli-max-tokens',\n","'bert-base-nli-cls-token',\n","'average_word_embeddings_levy_dependency',\n","'average_word_embeddings_komninos',\n","'average_word_embeddings_glove.840B.300d',\n","'average_word_embeddings_glove.6B.300d',\n","'allenai-specter',\n","'LaBSE']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oD3KXbmABSK8"},"outputs":[],"source":["len(xflist)"]},{"cell_type":"code","source":["pathname = '/content/gdrive/MyDrive/Data/text_df.csv'\n","\n","text_df.to_csv(pathname, index=False, encoding='utf-8-sig')\n","lhs = text_df[\"LHS\"].values\n","rhs = text_df[\"RHS\"].values\n","randtext = text_df[\"RandText\"].values\n","del text_df"],"metadata":{"id":"9MUH_sccC_kc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x5AaswgokTje"},"outputs":[],"source":["# Now process similarity values for each transformer in the list\n","import time\n","\n","# The notebook doesn't have enough disk to process all 113 models.\n","# So do a range in any given run. Modify the following two lines\n","# to cover the range you want.\n","#startm = 0\n","startm = 0\n","#endm = len(xflist)\n","endm = 113\n","\n","#time_list = []\n","\n","# Create Log File for each Transformer\n","log_df = pd.DataFrame(columns=['Transformer_Index','Transformer_Name',\"Time_Elapsed\"])\n","\n","pathname = '/content/gdrive/MyDrive/Data/text_df.csv'\n","\n","# Work our way through the list of transformers\n","for i in range(startm, endm):\n","  print(\"Model:\", i)\n","  t0 = time.perf_counter() # Time the sentence process: Capture the start time\n","\n","  model = SentenceTransformer(xflist[i])\n","\n","  # Compute sentence summaries: Model encode will handle a list of text values\n","  vleft = model.encode(lhs)\n","  vright = model.encode(rhs)\n","  vrand = model.encode(randtext)\n","\n","  del model # Recoup the memory\n","\n","  # Now compute similarities: There are three for each entry in the df\n","  lr_list = [] # Distance from left to right\n","  lrand_list = [] # Distance from left to rand\n","  rrand_list = [] # Distance from righ to rand\n","\n","  for j in range(text_samps):\n","    #a = vleft[j].reshape(1, -1)\n","    #b = vright[j].reshape(1, -1)\n","    lr_list.append(cosine_similarity(vleft[j].reshape(1, -1), vright[j].reshape(1, -1))[0][0])\n","\n","    #a = vleft[j].reshape(1, -1)\n","    #b = vrand[j].reshape(1, -1)\n","    lrand_list.append(cosine_similarity(vleft[j].reshape(1, -1), vrand[j].reshape(1, -1))[0][0])\n","\n","    #a = vright[j].reshape(1, -1)\n","    #b = vrand[j].reshape(1, -1)\n","    rrand_list.append(cosine_similarity(vright[j].reshape(1, -1), vrand[j].reshape(1, -1))[0][0])\n","\n","  log_df.loc[len(log_df)] = [i, xflist[i], time.perf_counter()-t0]\n","  #t1 = time.perf_counter()\n","  #time_list.append(t1 - t0)\n","  \n","  text_df = pd.read_csv(pathname)\n","  # Finally, add the columns to the dataset\n","  text_df.insert(4, \"RRANDsim\" + str(i), rrand_list, False)\n","  text_df.insert(4, \"LRANDsim\" + str(i), lrand_list, False)\n","  text_df.insert(4, \"LRsim\" + str(i), lr_list, False)\n","\n","  text_df.to_csv(pathname, index=False, encoding='utf-8-sig')\n","  del text_df"]},{"cell_type":"code","source":["log_df"],"metadata":{"id":"BEgkeB-pBVru"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_df"],"metadata":{"id":"wkguxqJanuwG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZkS_9IEtCRv"},"outputs":[],"source":["from google.colab import files\n","\n","text_df.to_csv(\"ppdb_sim_rest.csv\")\n","files.download(\"ppdb_sim_rest.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3bBLqsZ5vks"},"outputs":[],"source":["log_df.to_csv(\"ppdb_sim_timelog_0_34.csv\")\n","files.download(\"ppdb_sim_timelog_0_34.csv\")"]},{"cell_type":"code","source":["run_time_df = pd.DataFrame(columns=['Model_Index','Model_Name','Run_Time','Run_Type'])"],"metadata":{"id":"8oSJTu4h45cR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate consistency of run time\n","\n","# BY PICKING RANDOM MODEL AND LOADING THE MODEL IN EVERY ITERATION\n","import random\n","model_index_to_test = random.randint(0,len(xflist)-1)\n","print(\"Model:\", model_index_to_test)\n","\n","for iteration in range(100):\n","  print(\"Iteration:\", iteration)\n","  t0 = time.perf_counter() # Time the sentence process: Capture the start time\n","\n","  model = SentenceTransformer(xflist[model_index_to_test])\n","\n","  # Compute sentence summaries: Model encode will handle a list of text values\n","  vleft = model.encode(lhs)\n","  vright = model.encode(rhs)\n","  vrand = model.encode(randtext)\n","\n","  del model # Recoup the memory\n","\n","  # Now compute similarities: There are three for each entry in the df\n","  lr_list = [] # Distance from left to right\n","  lrand_list = [] # Distance from left to rand\n","  rrand_list = [] # Distance from righ to rand\n","\n","  for j in range(text_samps):\n","    lr_list.append(cosine_similarity(vleft[j].reshape(1, -1), vright[j].reshape(1, -1))[0][0])\n","    lrand_list.append(cosine_similarity(vleft[j].reshape(1, -1), vrand[j].reshape(1, -1))[0][0])\n","    rrand_list.append(cosine_similarity(vright[j].reshape(1, -1), vrand[j].reshape(1, -1))[0][0])\n","\n","  run_time_df.loc[len(run_time_df)] = [model_index_to_test, xflist[model_index_to_test], time.perf_counter()-t0, 'model_reloaded_in_every_iteration']"],"metadata":{"id":"ekcfYQQfUd1q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate consistency of run time\n","\n","# BY PICKING A MODEL AND LOADING THE MODEL JUST ONCE AT THE START OF THE LOOP\n","# I have used the 2 models for which I ran the above random model run time test\n","# This was to check the run time effect of loading the model\n","for model_index_to_test in [25]:\n","  print(\"Model:\", model_index_to_test)\n","  model = SentenceTransformer(xflist[model_index_to_test])\n","\n","  for iteration in range(100):\n","    print(\"Iteration:\", iteration)\n","    t0 = time.perf_counter() # Time the sentence process: Capture the start time\n","\n","    # Compute sentence summaries: Model encode will handle a list of text values\n","    vleft = model.encode(lhs)\n","    vright = model.encode(rhs)\n","    vrand = model.encode(randtext)\n","\n","    # Now compute similarities: There are three for each entry in the df\n","    lr_list = [] # Distance from left to right\n","    lrand_list = [] # Distance from left to rand\n","    rrand_list = [] # Distance from righ to rand\n","\n","    for j in range(text_samps):\n","      lr_list.append(cosine_similarity(vleft[j].reshape(1, -1), vright[j].reshape(1, -1))[0][0])\n","      lrand_list.append(cosine_similarity(vleft[j].reshape(1, -1), vrand[j].reshape(1, -1))[0][0])\n","      rrand_list.append(cosine_similarity(vright[j].reshape(1, -1), vrand[j].reshape(1, -1))[0][0])\n","\n","    run_time_df.loc[len(run_time_df)] = [model_index_to_test, xflist[model_index_to_test], time.perf_counter()-t0, 'model_loaded_once_before_iteration']"],"metadata":{"id":"ET-RqgiKb9fp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["run_time_df"],"metadata":{"id":"LOEahrq_2PyU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","\n","run_time_df.to_csv(\"model_run_time_stats.csv\")\n","files.download(\"model_run_time_stats.csv\")"],"metadata":{"id":"1rpg3Eldf-FA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"9iEMkhhUgO4Z"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"PPDBprocessorMultiBERT_KR.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}